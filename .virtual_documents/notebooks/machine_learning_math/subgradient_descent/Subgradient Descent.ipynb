get_ipython().run_line_magic("matplotlib", " inline")
import numpy as np
import matplotlib.pyplot as plt


# from the gradient descent for linear regression article
def create_function(theta):
    def f(X_b):
        return np.dot(X_b,theta)
    
    return f

# from the gradient descent for linear regression article
def add_intercept_ones(X):
    intercept_ones = np.ones((len(X),1)) # results in array( [ [1],..,[1] ] )
    X_b = np.c_[intercept_ones,X]
    return X_b

# from the standardization article
def standardize(X_train, X_test):
    mean = np.mean(X_train)
    std = np.std(X_train)
    X_train_s = (X_train - mean) / std 
    X_test_s = (X_test - mean) / std 
    return X_train_s, X_test_s, mean, std


X = np.array([1.25, 1.  , 0.75, 1.5, 1.75, 1.5 , 0.75])
y = np.array([40. , 42. , 46. , 37., 40. , 38. , 39.8])

X_train, X_test = X[:4], X[4:]
y_train, y_test = y[:4], y[4:]

X_train_s, X_test_s, _, _ = standardize(X_train, X_test)


plt.scatter(X_train, y_train, label="train")
plt.scatter(X_test, y_test, label="test")
plt.xlabel("age (years)")
plt.ylabel("price (â‚¬)")
plt.title("Figure Prices")
plt.grid(color = 'white', alpha=0.5, linestyle = '--', linewidth = 0.5)
plt.legend()
plt.show()


def get_elastic_mse_function(a2=1, a1=0):
    
    def elastic_mse(y, y_predicted, theta):
        error = y-y_predicted
        loss = 1/(y.size) * np.dot(error.T, error) + a2 * np.dot(theta,theta) + a1 * np.sum(np.abs(theta))
        return loss
    
    return elastic_mse


def get_elastic_gradient_function(a2=1, a1=0):
    
    def elastic_gradient(X_b, y, y_pred, theta):
        return  -(2/y.size) * X_b.T.dot((y - y_pred)) + a1 * np.sign(theta) + 2 * a2 * theta

    return elastic_gradient


def subgradient_descent(X, y, theta, criterion, subgradient_function, number_of_iterations, learning_rate):
    X_b = add_intercept_ones(X)
    for i in range(number_of_iterations):

        # predict and calculate loss
        f = create_function(theta) # create the current function
        y_predicted = f(X_b) # predict our entire x
        loss = criterion(y, y_predicted, theta) # calculate the error

        # perform optimization
        subgradient = subgradient_function(X_b, y, y_predicted, theta) # calculate gradient
        theta = theta - learning_rate * subgradient #adjust m and b
        
    return theta


theta = np.random.rand(2)
number_of_iterations = 3000
learning_rate = 0.01

# important: use standardized data!
theta_lasso_subgd = subgradient_descent(X_train_s, y_train, theta,
                    get_elastic_mse_function(a2=0, a1=1),
                    get_elastic_gradient_function(a2=0, a1=1),
                    number_of_iterations, learning_rate)
print(theta_lasso_subgd)


from sklearn.linear_model import Lasso


lasso_sklearn = Lasso()
# sklearn expects X to be a 2D-array, so we expand the dimensionality of our X
# by using numpys "expand_dims"-command
# Again, use standardized data!
lasso_sklearn.fit(np.expand_dims(X_train_s,1), y_train)

print(lasso_sklearn.intercept_, lasso_sklearn.coef_[0])


X_s = np.concatenate([X_train_s, X_test_s])

X_interval = np.array([min(X_s), max(X_s)]) # we'll use these points to plot our linear functions

plt.plot(X_train_s, y_train, "s", markersize=12, label="train data")
plt.plot(X_test_s, y_test, ".", markersize=15, label="test data")

plt.plot()
plt.plot(X_interval, add_intercept_ones(X_interval).dot(theta_lasso_subgd), label="lasso | subGD")
plt.plot(X_interval, lasso_sklearn.predict(np.expand_dims(X_interval,1)), label="lasso | sklearn")
plt.grid(color = 'white', alpha=0.5, linestyle = '--', linewidth = 0.5)
plt.legend()


from sklearn.linear_model import LinearRegression


linreg_sklearn = LinearRegression()
# here we technically don't need to use the standardized data,
# but since we're comparing different models it makes sense
# to use the same dataset
linreg_sklearn.fit(np.expand_dims(X_train_s,1), y_train)

print("Lasso:", theta_lasso_subgd)
print("OLS:", linreg_sklearn.intercept_, linreg_sklearn.coef_[0])


X_interval = np.array([min(X_s), max(X_s)]) # we'll use these points to plot our linear functions

plt.plot(X_train_s, y_train, "s", markersize=12, label="train data")
plt.plot(X_test_s, y_test, ".", markersize=15, label="test data")

plt.plot()
plt.plot(X_interval, add_intercept_ones(X_interval).dot(theta_lasso_subgd), label="lasso | subGD")
plt.plot(X_interval, linreg_sklearn.predict(np.expand_dims(X_interval,1)), label="OLS regression")
plt.grid(color = 'white', alpha=0.5, linestyle = '--', linewidth = 0.5)
plt.legend()


# This supresses the scientific notation in NumPy; purely cosmetic change
np.set_printoptions(suppress=True)


theta_rand = np.random.rand(51)
X_rand = np.random.rand(100,50)
y_rand = np.random.rand(100)
theta_rand_lasso_subgd = subgradient_descent(X_rand, y_rand, theta_rand,
                                     get_elastic_mse_function(a2=0, a1=1),
                                     get_elastic_gradient_function(a2=0, a1=1),
                                     3000, 0.01)
print(theta_rand_lasso_subgd)


def subgradient_descent(X, y, theta, criterion, subgradient_function, number_of_iterations, learning_rate):
    X_b = add_intercept_ones(X)
    theta_hist = []
    for i in range(1, number_of_iterations+1):
        if i > 20:
            learning_rate = 1 / (4 * i)

        # predict and calculate loss
        f = create_function(theta) # create the current function
        y_predicted = f(X_b) # predict our entire x
        loss = criterion(y, y_predicted, theta) # calculate the error

        # perform optimization
        subgradient = subgradient_function(X_b, y, y_predicted, theta) # calculate gradient
        theta = theta - learning_rate * subgradient #adjust m and b
        theta_hist.append(theta)
        
    return theta, theta_hist


load_dataset = True
if load_dataset:
    X_rand = np.load("./data/X_rand.np.npy")
    y_rand = np.load("./data/y_rand.np.npy")
    theta_rand = np.load("./data/theta_rand.np.npy")
else:
    np.random.seed(42)
    theta_rand = np.random.rand(2)
    X_rand = np.random.rand(100,1)
    y_rand = np.random.rand(100)


plt.scatter(X_rand, y_rand)
plt.xlabel("X")
plt.ylabel("y")
plt.title("Random dataset")
plt.grid(color = 'white', alpha=0.6, linestyle = '--', linewidth = 0.5)
plt.show()


theta_rand_optimized, theta_rand_hist = subgradient_descent(X_rand, y_rand, theta_rand,
                                     get_elastic_mse_function(a2=0, a1=1),
                                     get_elastic_gradient_function(a2=0, a1=1),
                                     1000, 0.01)
theta_rand_hist = np.array(theta_rand_hist)


import matplotlib.patheffects as pe


# Note: Running this cell will store a lot of files
# to run this cell, change the above to "if True"
if False:
    for i in range(len(theta_rand_hist)):
        if i < 100 or (i < 1000 and i % 50 == 0):

            plt.contourf(w_1t, w_2t, z, levels=500)#, cmap=plt.cm.jet)
            cont = plt.contour(w_1t, w_2t, z, levels=10, colors='white')
            plt.clabel(cont, inline=1)
            plt.scatter([0],[0], marker="x", color="cyan", s=150, zorder=100)
            plt.plot(theta_rand_hist[:,0][:i], theta_rand_hist[:,1][:i], color="red", marker="x")
            plt.annotate(
                f"iteration {i}",
                xy=(theta_rand_hist[i,0], theta_rand_hist[i,1]),
                xytext=(theta_rand_hist[i,0], theta_rand_hist[i,1] + 0.1),
                ha="center",
                arrowprops=dict(facecolor="black", shrink=0.1),
                color="white",
                fontsize="14",
                zorder=10000,
                path_effects=[pe.withStroke(linewidth=2, foreground="black")]
            )
            plt.savefig(f"./anim/subgd_frame_{i}.png")
            plt.close()


# Note: running this cell will store a lot of files
# to run this cell, change the above to "if True"

if False:
    # this is needed to further divide the zoom transition at the very end, because the numbers get so small
    scale_constants = [0.9, 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.004, 0.003, 0.002, 0.001, 0.00075, 0.0005, 0.00025]
    num_zoom_indices = len(zoomLevelsXMin) - 1 
    last_fixed_zoom_idx = num_zoom_indices - 1

    for i in range(len(zoomLevelsXMin) + len(cs) - 1):

            plt.contourf(w_1t, w_2t, z, levels=500)#, cmap=plt.cm.jet)
            cont = plt.contour(w_1t, w_2t, z, levels=10, colors='white')
            plt.clabel(cont, inline=1)
            plt.scatter([0],[0], marker="x", color="cyan", s=150, zorder=100)
            plt.plot(theta_rand_hist[:,0], theta_rand_hist[:,1], color="red", marker="x")

            if i < 49:
                plt.xlim(zoomLevelsXMin[i], zoomLevelsXMax[i])
                plt.ylim(zoomLevelsYMin[i], zoomLevelsYMax[i])
            else:
                plt.xlim(zoomLevelsXMin[last_fixed_zoom_idx] * scale_constants[i - num_zoom_indices], zoomLevelsXMax[last_fixed_zoom_idx] * cs[i - num_zoom_indices])
                plt.ylim(zoomLevelsYMin[last_fixed_zoom_idx] * scale_constants[i - num_zoom_indices], zoomLevelsYMax[last_fixed_zoom_idx] * cs[i - num_zoom_indices])

            plt.savefig(f"./anim2/subgd_frame_{i}.png")
            plt.close()


def subgradient_descent(X, y, theta, criterion, subgradient_function, number_of_iterations, learning_rate, eps):
    X_b = add_intercept_ones(X)
    for i in range(number_of_iterations):

        # predict and calculate loss
        f = create_function(theta) # create the current function
        y_predicted = f(X_b) # predict our entire x
        loss = criterion(y, y_predicted, theta) # calculate the error

        # perform optimization
        subgradient = np.array( subgradient_function(X_b, y, y_predicted, theta) ) # calculate gradient
        theta = theta - learning_rate * subgradient #adjust m and b
        
        theta[theta < eps] = 0
        
    return theta


theta_rand_lasso_subgd = subgradient_descent(X_rand, y_rand, theta_rand,
                                     get_elastic_mse_function(a2=0, a1=1),
                                     get_elastic_gradient_function(a2=0, a1=1),
                                     3000, 0.01, 0.01)
print(theta_rand_lasso_subgd)
